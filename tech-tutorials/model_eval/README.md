# Model Evaluation and Bias in Data Science Projects

We have 6 lessons - two sessions a day over three days. One session a day (or 
the latter half of the second session in a given day) should be hands-on. We 
can give homework, which could simply be the 'leftovers' of a given day's 
hands-on session.

- Day 1
 1. introduction, DSaPP-style projects, temporal data, spatiotemporal features.
    evaluation metrics, top-k, list stability, deployment.
 2. simple temporal cross-validation (label window, feature window, binary 
    outcome coercion); exercise on dataset with some pre-built features
- Day 2
 3. details on temporal cross-validation (sliding windows, sub-sampling of 
    training splits, (...?...) )
 4. model evaluation in practice (Cincinnati?); trade-offs; notebooks.
    Exercise.
- Day 3
 5. Bias Formulations
 6. Bias Practical (?)
